{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0pDTqIOW4Dp"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eyme2AIeW4Ds"
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "Learned probabilities\n",
        "\n",
        "Given the following run of the Naive Bayes algorithm without smoothing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NymrDW7PW4Dt",
        "outputId": "c2608165-2804-42f4-b8f3-ae8468b3569a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Log Prior: [-1.09861229 -0.40546511]\n",
            "Feature Log Probabilities: [[ -0.28768207  -1.38629436  -1.38629436]\n",
            " [-23.02585093   0.          -0.69314718]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_class_log_prior(messages):\n",
        "    class_counts = messages['is_spam'].value_counts().sort_index()\n",
        "    class_probs = class_counts / len(messages)\n",
        "    class_log_prior = np.log(class_probs)\n",
        "    return class_log_prior.values\n",
        "\n",
        "def calculate_feature_log_prob(messages):\n",
        "    class_0 = messages[messages['is_spam'] == 0]\n",
        "    class_1 = messages[messages['is_spam'] == 1]\n",
        "\n",
        "    feature_probs_class_0 = class_0.iloc[:, :-1].sum() / len(class_0)\n",
        "    feature_probs_class_1 = class_1.iloc[:, :-1].sum() / len(class_1)\n",
        "\n",
        "    feature_probs_class_0 = feature_probs_class_0.replace(0, 1e-10)\n",
        "    feature_probs_class_1 = feature_probs_class_1.replace(0, 1e-10)\n",
        "\n",
        "    feature_log_prob_class_0 = np.log(feature_probs_class_0)\n",
        "    feature_log_prob_class_1 = np.log(feature_probs_class_1)\n",
        "\n",
        "    feature_log_prob = np.vstack([feature_log_prob_class_0, feature_log_prob_class_1])\n",
        "    return feature_log_prob\n",
        "\n",
        "messages = pd.DataFrame(\n",
        "    [(1, 0, 0, 0),\n",
        "     (0, 0, 1, 0),\n",
        "     (1, 0, 0, 0),\n",
        "     (1, 1, 0, 0)] +\n",
        "    [(0, 1, 0, 1)] * 4 +\n",
        "    [(0, 1, 1, 1)] * 4,\n",
        "    columns=['study', 'free', 'money', 'is_spam']\n",
        ")\n",
        "\n",
        "class_log_prior = calculate_class_log_prior(messages)\n",
        "feature_log_prob = calculate_feature_log_prob(messages)\n",
        "\n",
        "print(\"Class Log Prior:\", class_log_prior)\n",
        "print(\"Feature Log Probabilities:\", feature_log_prob)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWShL7oOW4Dv"
      },
      "source": [
        "1. Write a function that independently calculates the value of the `class_log_prior_` attribute without smoothing using only `messages` as parameter. (These are the natural logarithms of class probabilities $P(v_j)$).\n",
        "2. Write a function that independently calculates the value of the `feature_log_prob_` attribute without smoothing using only `messages` as parameter. (These are the natural logarithms of attribute probabilities $P(a_i|v_j)$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CODkp4vW4Dv"
      },
      "source": [
        "## Exercise 2\n",
        "Expected error rate in training\n",
        "\n",
        "Consider a binary classification problem with features $X_1$ and $X_2$ and label $Y$. The two features are assumed to be conditionally independent with respect to $Y$ . The prior probabilities $P(Y=0)$ and $P(Y=1)$ are both equal to 0.5. The conditional probabilities are:\n",
        "\n",
        "<table style=\"float: left;\">\n",
        "    <tr>\n",
        "        <th>P(X1|Y)</th>\n",
        "        <th>Y=0</th>\n",
        "        <th>Y=1</th></tr>\n",
        "    <tr>\n",
        "        <th>X1=0</th>\n",
        "        <td>0.7</td>\n",
        "        <td>0.2</td></tr>\n",
        "    <tr>\n",
        "        <th>X1=1</th>\n",
        "        <td>0.3</td>\n",
        "        <td>0.8</td>\n",
        "    </tr>\n",
        "</table>\n",
        "\n",
        "<table style=\"float: left; margin-left: 20px;\">\n",
        "    <tr>\n",
        "        <th>P(X2|Y)</th>\n",
        "        <th>Y=0</th>\n",
        "        <th>Y=1</th></tr>\n",
        "    <tr>\n",
        "        <th>X2=0</th>\n",
        "        <td>0.9</td>\n",
        "        <td>0.5</td></tr>\n",
        "    <tr>\n",
        "        <th>X2=1</th>\n",
        "        <td>0.1</td>\n",
        "        <td>0.5</td>\n",
        "    </tr>\n",
        "</table>\n",
        "\n",
        "<br style=\"clear: left;\" />\n",
        "\n",
        "1. Generate a `DataFrame` with 1000 entries and three columns `['x1', 'x2', 'y']`, according to the description above, using the `bernoulli.rvs` function from `scipy`.\n",
        "1. After training on the DataFrame above, predict every combination of values for $X_1$ and $X_2$.\n",
        "1. Calculate the average error rate on the training dataset.\n",
        "1. Create a new attribute $X_3$ as a copy of $X_2$. What is the new average error rate on the training dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF49BZXpW4Dv"
      },
      "source": [
        "## Exercise 3\n",
        "Joint Bayes\n",
        "\n",
        "Considering the dataset below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2875d91EW4Dw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tools.pd_helpers import apply_counts\n",
        "\n",
        "d = pd.DataFrame({'X1': [0, 0, 1, 1, 0, 0, 1, 1],\n",
        "                  'X2': [0, 0, 0, 0, 1, 1, 1, 1],\n",
        "                  'C' : [2, 18, 4, 1, 4, 1, 2, 18],\n",
        "                  'Y' : [0, 1, 0, 1, 0, 1, 0, 1]})\n",
        "d=apply_counts(d, 'C')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9r-pvVEW4Dw"
      },
      "source": [
        "1. Implement a simple version of the Joint Bayes algorithm by creating the `BernoulliJB` class, similar to `BernoulliNB` from `scikit`, but only implement the `fit(X,y)` and `predict_proba(X)` without smoothing.\n",
        "1. How many probabilities are estimated by the the Joint Bayes algorithm?\n",
        "1. What are probability estimates for the instance $X_1 = 0$, $X_2 = 0$ calculated by `predict_proba(X)` from `BernoulliJB`?\n",
        "1. What are the predicted probabilities of Naive Bayes (using `predict_proba(X)` from `BernoulliNB`) without smoothing for this instance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR5lS_XNW4Dx"
      },
      "source": [
        "## Exercise 4\n",
        "Measuring the naivety assumption\n",
        "\n",
        "Consider a simple text classification that only considers two words: $w_1$ and $w_2$. The label $y$ will only be 1 if $w_1$ is present and $w_2$ is not, so the label is effectively the function $w1 \\land \\lnot w2$.\n",
        "\n",
        "The `correlated_df` function below will return such a dataset, with 10,000 entries and the columns `['w1', 'w2', 'y']`. The parameter `corr` specifies approximately how much correlation should exist between `w1` and `w2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xWIHQQOW4Dx",
        "outputId": "8bd466d5-f5ad-46de-91eb-aadcbe9f45f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correlation:  PearsonRResult(statistic=np.float64(0.469833135691177), pvalue=np.float64(0.0))\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import bernoulli\n",
        "from scipy.stats import pearsonr\n",
        "size = 10000\n",
        "\n",
        "def correlated_df(corr):\n",
        "    w1 = bernoulli.rvs(0.5, size=size, random_state=1)\n",
        "    d = pd.DataFrame({'w1': w1})\n",
        "    mask = bernoulli.rvs(corr, size=size, random_state=2)\n",
        "    random = bernoulli.rvs(0.5, size=size, random_state=3)\n",
        "    d['w2'] = d['w1'] & mask | random & ~mask\n",
        "    d['mask'] = mask\n",
        "    d['random'] = random\n",
        "    d['y'] = d['w1'] & ~ d['w2']\n",
        "    return d\n",
        "\n",
        "d = correlated_df(0.5)\n",
        "\n",
        "# Check that the correlation is indeed close to 0.5\n",
        "print(\"Correlation: \", pearsonr(d['w1'], d['w2']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFEs-REIW4Dx"
      },
      "source": [
        "1. With the function above, create a line chart using `matplotlib` that shows how the correlation affects the training error of Naive Bayes (no smoothing).\n",
        "1. Using the function above, create a line chart that shows how the correlation affects the training error of a decision tree classifier (see the `DecisionTreeClassifier` class from `sklearn`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8SbHAXmW4Dx"
      },
      "source": [
        "## Exercise 5\n",
        "Average error rate\n",
        "\n",
        "Given the function $Y = (A \\land B) \\lor \\neg(B \\lor C)$ where $A$, $B$ and $C$ are independent binary random variables, each of which having 50% chance of being 0 and 50% chance of being 1.\n",
        "\n",
        "1. Generate a DataFrame with 1000 entries and four columns `A`, `B`, `C` and `Y`, according to the description above, using the `bernoulli.rvs` function from `scipy`.\n",
        "1. Calculate the error rate for Naive Bayes on the training dataset.\n",
        "1. What is the average error rate on this training dataset for the Joint Bayes algorithm? (Note that you don't have to actually build the algorithm, just provide a theoretical justification.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import bernoulli\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "A = bernoulli.rvs(0.5, size=1000)\n",
        "B = bernoulli.rvs(0.5, size=1000)\n",
        "C = bernoulli.rvs(0.5, size=1000)\n",
        "\n",
        "Y = (A & B) | ~(B | C)\n",
        "\n",
        "Y = Y.astype(int)\n",
        "\n",
        "df = pd.DataFrame({'A': A, 'B': B, 'C': C, 'Y': Y})\n",
        "\n",
        "X = df[['A', 'B', 'C']]\n",
        "y = df['Y']\n",
        "\n",
        "nb_model = BernoulliNB()\n",
        "nb_model.fit(X, y)\n",
        "\n",
        "y_pred = nb_model.predict(X)\n",
        "\n",
        "accuracy = accuracy_score(y, y_pred)\n",
        "error_rate = 1 - accuracy\n",
        "print(f\"Naive Bayes Error Rate: {error_rate}\")\n"
      ],
      "metadata": {
        "id": "5mmhAOe7YghY",
        "outputId": "d1dea7cb-b279-4ec3-f2a4-8d834b2c942c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Error Rate: 0.236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvT8OuzJW4Dy"
      },
      "source": [
        "## Exercise 6\n",
        "Text classification\n",
        "\n",
        "A news company would like to automatically sort the news articles related to sport from those related to politics. They are using 8 key words ($w_1,...,w_8)$ and have annotated several articles in each category for training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrCk-zA3W4Dy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "features = [f'w{i}' for i in range(1,9)]\n",
        "\n",
        "politics=pd.DataFrame([\n",
        "(1, 0, 1, 1, 1, 0, 1, 1),\n",
        "(0, 0, 0, 1, 0, 0, 1, 1),\n",
        "(1, 0, 0, 1, 1, 0, 1, 0),\n",
        "(0, 1, 0, 0, 1, 1, 0, 1),\n",
        "(0, 0, 0, 1, 1, 0, 1, 1),\n",
        "(0, 0, 0, 1, 1, 0, 0, 1)],\n",
        "columns=features)\n",
        "\n",
        "sport=pd.DataFrame([\n",
        "(1, 1, 0, 0, 0, 0, 0, 0),\n",
        "(0, 0, 1, 0, 0, 0, 0, 0),\n",
        "(1, 1, 0, 1, 0, 0, 0, 0),\n",
        "(1, 1, 0, 1, 0, 0, 0, 1),\n",
        "(1, 1, 0, 1, 1, 0, 0, 0),\n",
        "(0, 0, 0, 1, 0, 1, 0, 0),\n",
        "(1, 1, 1, 1, 1, 0, 1, 0)],\n",
        "columns=features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB-OE35kW4Dy"
      },
      "source": [
        "According to Naive Bayes (without smoothing), what is the probability that the document `x = (1, 0, 0, 1, 1, 1, 1, 0)` is about politics?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}